% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iVAE.R
\name{iVAE}
\alias{iVAE}
\title{Trains an identifiable variational autoencoder (iVAE) using the input data.}
\usage{
iVAE(
  data,
  aux_data,
  latent_dim,
  test_data = NULL,
  test_data_aux = NULL,
  hidden_units = c(128, 128, 128),
  aux_hidden_units = c(128, 128, 128),
  activation = "leaky_relu",
  source_dist = "gaussian",
  validation_split = 0,
  error_dist = "gaussian",
  error_dist_sigma = 0.02,
  optimizer = NULL,
  lr_start = 0.001,
  lr_end = 1e-04,
  steps = 10000,
  seed = NULL,
  get_prior_means = TRUE,
  true_data = NULL,
  epochs,
  batch_size
)
}
\arguments{
\item{data}{A matrix with P columns and N rows containing the observed data.}

\item{aux_data}{A matrix with D columns and N rows
containing the auxiliary data.}

\item{latent_dim}{A latent dimension for iVAE}

\item{hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in encoder and K layers in decoder.}

\item{aux_hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in auxiliary function.}

\item{activation}{Activation function for the hidden layers.}

\item{source_dist}{Distribution for the latent source.
Either "gaussian" or "laplace".}

\item{validation_split}{Proportion of data used for validation}

\item{error_dist}{Distribution for the model error.
Either "gaussian" or "laplace".}

\item{error_dist_sigma}{A standard deviation for error_dist.}

\item{optimizer}{A keras optimizer for the tensorflow model.
A default is Adam optimizer with polynomial decay.}

\item{lr_start}{A starting learning rate for the default optimizer.}

\item{lr_end}{A ending learning rate for the default optimizer.}

\item{steps}{A number of learning steps between lr_start and lr_end
for the default optimizer.}

\item{seed}{Seed for the tensorflow model. Should be used instead of
set.seed(seed).}

\item{get_prior_means}{A boolean defining if the means provided by
the auxiliary function are returned.}

\item{true_data}{The true latent components. If provided, the mean
correlation coefficient is calculated for each epoch.}

\item{epochs}{A number of epochs for training.}

\item{batch_size}{A batch size for training.}
}
\value{
An object of class VAE.
\item{IC_unscaled}{Unscaled latent components.}
\item{IC}{The latent component with
zero mean and unit variance.}
\item{data_dim}{The dimension of the original data.}
\item{metrics}{Metrics of the training for each epoch.}
\item{sample_size}{Sample size of
the data.}
\item{call_params}{The params for
the original VAE method call.}
\item{encoder}{The trained encoder.}
\item{decoder}{The trained decoder.}
\item{IC_means}{The means of the
unscaled latent components.}
\item{IC_means}{The standard devivations
of the unscaled latent components.}
\item{prior_means}{Means provided by auxiliary function.
Scaled to match the normalized latent components.}
\item{prior_vars}{Variances provided by auxiliary function.
Scaled to match the normalized latent components.}
\item{prior_mean_model}{A model, which outputs the means estimated by
the auxiliary function.}
\item{MCCs}{Mean correlation coefficients for each epoch.
Provided if true_data is not NULL.}
\item{call}{The of how the method was called.}
\item{DNAME}{The of the original data.}
}
\description{
Trains an identifiable variational autoencoder (iVAE) using the input data.
}
\details{
The method constructs and trains an identifiable variational
autoencoder (iVAE) \insertRef{Khemakhem2020}{NonlinearBSS}
based on the given parameters.
iVAE is composed of an encoder, a decoder and an auxiliary function.
The encoder transforms the original data into a latent representation.
The decoder aims to transform the latent representation back to the
original data. The auxiliary function estimates the mean and the
variance of the data based on the auxiliary
data. The variational approximation is obtained by using a reparametrization
trick to sample a new value using the mean and the standard deviation
given by the encoder.
}
\examples{
p <- 3
n_segments <- 5
n_per_segment <- 100
n <- n_segments * n_per_segment
data <- matrix(NA, ncol = p, nrow = n_segments * n_per_segment)
aux_data <- matrix(NA, ncol = n_segments, nrow = n_segments * n)
# Create artificial data with variance and mean varying over the segments.
for (i in 1:p) {
  for (seg in 1:n_segments) {
    start_ind <- (seg - 1) * n_per_segment + 1
    end_ind <- seg * n_per_segment
    data[start_ind:end_ind, i] <- rnorm(
      n_per_segment,
      runif(1, -5, 5), runif(1, 0.1, 5)
    )
  }
}
mixed_data <- mix_data(data, 3, "elu")
res <- iVAE(data, aux_data, 3, epochs = 500, batch_size = 64)
}
\references{
\insertAllCited{}
}
