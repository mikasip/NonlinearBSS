% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iVAE.R
\name{iVAE}
\alias{iVAE}
\title{Identifiable Variational Autoencoder}
\usage{
iVAE(
  data,
  aux_data,
  latent_dim,
  hidden_units = c(128, 128, 128),
  aux_hidden_units = c(128, 128, 128),
  activation = "leaky_relu",
  source_dist = "gaussian",
  validation_split = 0,
  error_dist = "gaussian",
  error_dist_sigma = 0.02,
  optimizer = NULL,
  lr_start = 0.001,
  lr_end = 1e-04,
  get_elbo = TRUE,
  steps = 10000,
  seed = NULL,
  epochs,
  batch_size
)
}
\arguments{
\item{data}{A matrix with P columns and N rows containing the observed data.}

\item{aux_data}{A matrix with D columns and N rows
containing the auxiliary data.}

\item{latent_dim}{A latent dimension for iVAE}

\item{hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in encoder and K layers in decoder.}

\item{aux_hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in auxiliary function.}

\item{activation}{Activation function for the hidden layers.}

\item{source_dist}{Distribution for the latent source.
Either "gaussian" or "laplace".}

\item{validation_split}{Proportion of data used for validation}

\item{error_dist}{Distribution for the model error.
Either "gaussian" or "laplace".}

\item{error_dist_sigma}{A standard deviation for error_dist.}

\item{optimizer}{A keras optimizer for the tensorflow model.
A default is Adam optimizer with polynomial decay.}

\item{lr_start}{A starting learning rate for the default optimizer.}

\item{lr_end}{A ending learning rate for the default optimizer.}

\item{get_elbo}{Logical. If TRUE, the model returns the final ELBO value. Default is FALSE.}

\item{steps}{A number of learning steps between lr_start and lr_end
for the default optimizer.}

\item{seed}{Seed for the tensorflow model. Should be used instead of
set.seed(seed).}

\item{epochs}{A number of epochs for training.}

\item{batch_size}{A batch size for training.}
}
\value{
An object of class iVAE.
\item{IC_unscaled}{Unscaled latent variable estimates.}
\item{IC}{Scaled latent variable estimates.}
\item{data_dim}{Dimension of the observed data.}
\item{sample_size}{Sample size of the training data.}
\item{aux_dim}{The dimension of auxiliary variable vector.}
\item{prior_mean_model}{A model, which outputs the means estimated by
the auxiliary function.}
\item{prior_log_var_model}{A model, which outputs the logarithmic variances 
estimated by the auxiliary function.}
\item{encoder}{The trained encoder.}
\item{decoder}{The trained decoder.}
\item{data_means}{The means of the
original data.}
\item{data_sds}{The standard deviations of the
original data.}
\item{IC_means}{The means of the
unscaled latent components.}
\item{IC_sds}{The standard deviations of the
unscaled latent components.}
\item{call_params}{The params for
the original iVAE method call.}
\item{elbo}{The ELBO value after training the model.}
\item{metrics}{Metrics of the training for each epoch.}
\item{call}{An output of how the method was called.}
\item{DNAME}{The name of the original data.}
}
\description{
Trains an identifiable variational autoencoder
(iVAE) using the input data.
\loadmathjax
}
\details{
The method constructs and trains an identifiable variational
autoencoder (iVAE) \insertCite{Khemakhem2020}{NonlinearBSS}
based on the given parameters.
iVAE is composed of an encoder \mjeqn{g}{ascii}, a decoder
\mjeqn{h}{ascii} and
an auxiliary function \mjeqn{w}{ascii}.
The encoder transforms the original data
\mjeqn{x}{ascii} into a mean and a variace
vectors.
The mean and the variance are then used to sample a latent representation by
using a reparametrization trick \insertCite{kingma2013auto}{NonlinearBSS}.
The decoder aims to transform the latent representation
\mjeqn{z}{ascii} back to the
original data. The auxiliary function estimates the mean and the
variance of the data based on the auxiliary
data.
The functions \mjeqn{g}{ascii}, \mjeqn{h}{ascii}
and \mjeqn{w}{ascii} are deep neural networks
with parameters
\mjeqn{\lambda=(\lambda_g, \lambda_h, \lambda_w)^\top}{ascii}.
The parameters are learned by minimizing the lower bound of the data
log-likelihood:
\mjdeqn{
\mathcal{L}(\theta |  x,  u) \geq
E_{q_{ \theta_{ g}}( z| x, u)} (
log \, p_{ \theta_{ h}}(x | z)  +
log \, p_{ \theta_{w}}(z | u) -
log \, q_{\theta_{g}}(z | x, u) ).
}{ascii}

In the loss function, \mjeqn{ log \, p_{ \theta_{ h}}(x | z)}{ascii}
controls the reconstruction error, and have the distribution based on the
parameter \code{error_dist} where \code{gaussian} or
\code{laplace} are currently supported. The location of
the distribution is the original data and the scale parameter
is given by \code{error_dist_sigma}. The default value for
\code{error_dist_sigma} is \code{0.02}. By decreasing the value,
the loss function emphasizes more the reconstruction error and
by increasing, the reconstruction error has less weight.
The term \mjeqn{log \, p_{ \theta_{w}}(z | u) -
log \, q_{\theta_{g}}(z | x, u)}{ascii} tries to make the
distributions \mjeqn{p_{\theta_{w}}(z | u)}{ascii} and
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii} as similar as possible.
This term controls the disentangling and allows the method
to find the true latent components.
The parameter \code{source_dist} defines the distributions
\mjeqn{p_{ \theta_{w}}(z | u)}{ascii} and
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii}.
The distributions \code{gaussian} and \code{laplace} are currently
supported.
The parameters for \mjeqn{p_{ \theta_{w}}(z | u)}{ascii} are given by the
auxiliary function and the parameters for
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii} are given by the encoder.

The method is identifiable, meaning that it finds the true latent
representation in the limit of infinite data, if some conditions are
satisfied. Loosely speaking, in case of gaussian and laplace distributions,
the conditions are that the mixing
function is differentiable and that the variance (or scale)
of the latent sources are varying based on the auxiliary variable.
The exact identifiability results are given in
\insertCite{Khemakhem2020}{NonlinearBSS}.
}
\examples{
p <- 3
n_segments <- 10
n_per_segment <- 100
n <- n_segments * n_per_segment
latent_data <- matrix(NA, ncol = p, nrow = n)
aux_data <- matrix(0, ncol = n_segments, nrow = n)
# Create artificial data with variance and mean varying over the segments.
for (i in 1:p) {
  for (seg in 1:n_segments) {
    start_ind <- (seg - 1) * n_per_segment + 1
    end_ind <- seg * n_per_segment
    aux_data[start_ind:end_ind, seg] <- 1
    latent_data[start_ind:end_ind, i] <- rnorm(
      n_per_segment,
      runif(1, -5, 5), runif(1, 0.1, 5)
    )
  }
}
mixed_data <- mix_data(latent_data, 2, "elu")

# For better performance, increase the number of epochs
res <- iVAE(mixed_data, aux_data, 3, epochs = 10, batch_size = 64)
cormat <- cor(res$IC, latent_data)
cormat
absolute_mean_correlation(cormat)
}
\references{
\insertAllCited{}
}
\author{
Mika SipilÃ¤
}
