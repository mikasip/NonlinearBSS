% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iVAE.R
\name{iVAE}
\alias{iVAE}
\title{Identifiable Variational Autoencoder}
\usage{
iVAE(
  data,
  aux_data,
  latent_dim,
  test_data = NULL,
  test_data_aux = NULL,
  hidden_units = c(128, 128, 128),
  aux_hidden_units = c(128, 128, 128),
  activation = "leaky_relu",
  source_dist = "gaussian",
  validation_split = 0,
  error_dist = "gaussian",
  error_dist_sigma = 0.02,
  optimizer = NULL,
  lr_start = 0.001,
  lr_end = 1e-04,
  steps = 10000,
  seed = NULL,
  get_prior_means = TRUE,
  true_data = NULL,
  epochs,
  batch_size
)
}
\arguments{
\item{data}{A matrix with P columns and N rows containing the observed data.}

\item{aux_data}{A matrix with D columns and N rows
containing the auxiliary data.}

\item{latent_dim}{A latent dimension for iVAE}

\item{hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in encoder and K layers in decoder.}

\item{aux_hidden_units}{K-dimensional vector giving the number of
hidden units for K layers in auxiliary function.}

\item{activation}{Activation function for the hidden layers.}

\item{source_dist}{Distribution for the latent source.
Either "gaussian" or "laplace".}

\item{validation_split}{Proportion of data used for validation}

\item{error_dist}{Distribution for the model error.
Either "gaussian" or "laplace".}

\item{error_dist_sigma}{A standard deviation for error_dist.}

\item{optimizer}{A keras optimizer for the tensorflow model.
A default is Adam optimizer with polynomial decay.}

\item{lr_start}{A starting learning rate for the default optimizer.}

\item{lr_end}{A ending learning rate for the default optimizer.}

\item{steps}{A number of learning steps between lr_start and lr_end
for the default optimizer.}

\item{seed}{Seed for the tensorflow model. Should be used instead of
set.seed(seed).}

\item{get_prior_means}{A boolean defining if the means provided by
the auxiliary function are returned.}

\item{true_data}{The true latent components. If provided, the mean
correlation coefficient is calculated for each epoch.}

\item{epochs}{A number of epochs for training.}

\item{batch_size}{A batch size for training.}
}
\value{
An object of class VAE.
\item{IC_unscaled}{Unscaled latent components.}
\item{IC}{The latent component with
zero mean and unit variance.}
\item{aux_data}{A matrix containing the provided auxiliary data.}
\item{original_data}{A matrix containing the original data.}
\item{data_dim}{The dimension of the original data.}
\item{metrics}{Metrics of the training for each epoch.}
\item{sample_size}{Sample size of
the data.}
\item{call_params}{The params for
the original VAE method call.}
\item{encoder}{The trained encoder.}
\item{decoder}{The trained decoder.}
\item{IC_means}{The means of the
unscaled latent components.}
\item{IC_means}{The standard devivations
of the unscaled latent components.}
\item{prior_means}{Means provided by auxiliary function.
Scaled to match the normalized latent components.}
\item{prior_vars}{Variances provided by auxiliary function.
Scaled to match the normalized latent components.}
\item{prior_mean_model}{A model, which outputs the means estimated by
the auxiliary function.}
\item{MCCs}{Mean correlation coefficients for each epoch.
Provided if true_data is not NULL.}
\item{call}{The of how the method was called.}
\item{DNAME}{The of the original data.}
}
\description{
Trains an identifiable variational autoencoder
(iVAE) using the input data.
\loadmathjax{}
}
\details{
The method constructs and trains an identifiable variational
autoencoder (iVAE) \insertCite{Khemakhem2020}{NonlinearBSS}
based on the given parameters.
iVAE is composed of an encoder \mjeqn{g}{ascii}, a decoder
\mjeqn{h}{ascii} and
an auxiliary function \mjeqn{w}{ascii}.
The encoder transforms the original data
\mjeqn{x}{ascii} into a mean and a variace
vectors.
The mean and the variance are then used to sample a latent representation by
using a reparametrization trick \insertCite{kingma2013auto}{NonlinearBSS}.
The decoder aims to transform the latent representation
\mjeqn{z}{ascii} back to the
original data. The auxiliary function estimates the mean and the
variance of the data based on the auxiliary
data.
The functions \mjeqn{g}{ascii}, \mjeqn{h}{ascii}
and \mjeqn{w}{ascii} are deep neural networks
with parameters
\mjeqn{\lambda=(\lambda_g, \lambda_h, \lambda_w)^\top}{ascii}.
The parameters are learned by minimizing the lower bound of the data
log-likelihood:
\mjdeqn{
\mathcal{L}(\theta |  x,  u) \geq
E_{q_{ \theta_{ g}}( z| x, u)} (
\text{log}\, p_{ \theta_{ h}}(x | z)  +
\text{log}\,p_{ \theta_{w}}(z | u) -
\text{log}\,q_{\theta_{g}}(z | x, u) ).
}{ascii}

In the loss function, \mjeqn{\text{log} p_{ \theta_{ h}}(x | z)}{ascii}
controls the reconstruction error, and have the distribution based on the
parameter \code{error_dist} where \code{gaussian} or
\code{laplace} are currently supported. The location of
the distribution is the original data and the scale parameter
is given by \code{error_dist_sigma}. The default value for
\code{error_dist_sigma} is \code{0.02}. By decreasing the value,
the loss function emphasizes more the reconstruction error and
by increasing, the reconstruction error has less weight.
The term \mjeqn{\text{log}\,p_{ \theta_{w}}(z | u) -
\text{log}\,q_{\theta_{g}}(z | x, u)}{ascii} tries to make the
distributions \mjeqn{p_{\theta_{w}}(z | u)}{ascii} and
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii} as similar as possible.
This term controls the disentangling and allows the method
to find the true latent components.
The parameter \code{source_dist} defines the distributions
\mjeqn{p_{ \theta_{w}}(z | u)}{ascii} and
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii}.
The distributions \code{gaussian} and \code{laplace} are currently
supported.
The parameters for \mjeqn{p_{ \theta_{w}}(z | u)}{ascii} are given by the
auxiliary function and the parameters for
\mjeqn{q_{\theta_{g}}(z | x, u)}{ascii} are given by the encoder.

The method is identifiable, meaning that it finds the true latent
representation in the limit of infinite data, if some conditions are
satisfied. Loosely speaking, in case of gaussian and laplace distributions,
the conditions are that the mixing
function is differentiable and that the variance (or scale)
of the latent sources are varying based on the auxiliary variable.
The exact identifiability results are given in
\insertCite{Khemakhem2020}{NonlinearBSS}.
}
\examples{
p <- 3
n_segments <- 10
n_per_segment <- 100
n <- n_segments * n_per_segment
latent_data <- matrix(NA, ncol = p, nrow = n)
aux_data <- matrix(NA, ncol = n_segments, nrow = n)
# Create artificial data with variance and mean varying over the segments.
for (i in 1:p) {
  for (seg in 1:n_segments) {
    start_ind <- (seg - 1) * n_per_segment + 1
    end_ind <- seg * n_per_segment
    aux_data[start_ind:end_ind, seg] <- 1
    latent_data[start_ind:end_ind, i] <- rnorm(
      n_per_segment,
      runif(1, -5, 5), runif(1, 0.1, 5)
    )
  }
}
mixed_data <- mix_data(latent_data, 2, "elu")
res <- iVAE(mixed_data, aux_data, 3, epochs = 300, batch_size = 64)
cormat <- cor(res$IC, latent_data)
cormat
absolute_mean_correlation(cormat)
}
\references{
\insertAllCited{}
}
\author{
Mika SipilÃ¤
}
