% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iVAEar.R
\name{iVAEar}
\alias{iVAEar}
\title{Identifiable Variational Autoencoder with AR(R) Latent Structure}
\usage{
iVAEar(
  data,
  aux_data,
  latent_dim,
  prev_data_list,
  prev_aux_data_list,
  hidden_units = c(128, 128, 128),
  aux_hidden_units = c(128, 128, 128),
  activation = "leaky_relu",
  source_dist = "gaussian",
  validation_split = 0,
  error_dist = "gaussian",
  error_dist_sigma = 0.01,
  optimizer = NULL,
  lr_start = 0.001,
  lr_end = 1e-04,
  ar_order = 1,
  steps = 10000,
  seed = NULL,
  get_elbo = TRUE,
  epochs,
  batch_size
)
}
\arguments{
\item{data}{A numeric matrix of observed data (n x p), where n is the number of samples, 
and p is the number of features.}

\item{aux_data}{A numeric matrix of auxiliary data (n x m), where m is the number of auxiliary variables.}

\item{latent_dim}{An integer specifying the latent dimension.}

\item{prev_data_list}{A list of numeric matrices of previously observed data, 
used for constructing the AR(R) latent prior.}

\item{prev_aux_data_list}{A list of numeric matrices of auxiliary data for the previous observations.}

\item{hidden_units}{A numeric vector specifying the number of units in each hidden layer for the encoder and decoder.}

\item{aux_hidden_units}{A numeric vector specifying the number of units in each 
hidden layer in auxiliary function.}

\item{activation}{A string specifying the activation function to be used in hidden layers. Default is `"leaky_relu"`.}

\item{source_dist}{A character string specifying the distribution for the latent variables. Choices are `"gaussian"` (default) or `"laplace"`.}

\item{validation_split}{A numeric value specifying the fraction of training data to use for validation during model keras3::fitting. Default is 0.}

\item{error_dist}{A string specifying the distribution for the reconstruction error. Choices are `"gaussian"` (default) or `"laplace"`.}

\item{error_dist_sigma}{A numeric value specifying the standard deviation for the error distribution. Default is 0.01.}

\item{optimizer}{An optional Keras optimizer object. If NULL (default), an Adam optimizer with polynomial decay is used.}

\item{lr_start}{A numeric value for the initial learning rate. Default is 0.001.}

\item{lr_end}{A numeric value for the final learning rate. Default is 0.0001.}

\item{ar_order}{An autoregressive order used in iVAEar.}

\item{steps}{An integer specifying the number of optimization steps for polynomial learning rate decay. Default is 10,000.}

\item{seed}{An optional integer for setting the random seed to ensure reproducibility. Default is NULL.}

\item{get_elbo}{Logical. If TRUE, the model returns the final ELBO value. Default is FALSE.}

\item{epochs}{An integer specifying the number of training epochs.}

\item{batch_size}{An integer specifying the batch size for training.}
}
\value{
#' A keras3::fitted iVAEar object of class \code{iVAEar}, which inherits from class \code{\link{iVAE}}.
In addition the object has the following field:
\item{prior_ar_model}{A model, which outputs the estimated AR coefficients
by the auxiliary function.}
}
\description{
This function keras3::fits an Identifiable Variational Autoencoder (iVAE) model where the latent 
variables follow an AR(R) process, R being the autoregressive order.
It also supports handling auxiliary data and accommodates various 
choices for source and error distributions.
}
\details{
The iVAEar method extends spatio-temporal identifiable variational 
autoencoders (\code{\link{iVAE}}) with an autoregressive (AR) structure. It consists of:
- Encoder \mjeqn{\mathbf{g}(\mathbf{x}, \mathbf{u})}{ascii}: Maps observations 
\mjeqn{\mathbf{x}}{ascii} and auxiliary variables \mjeqn{\mathbf{u}}{ascii} to 
latent variables \mjeqn{\mathbf{z}}{ascii}.

- Decoder \mjeqn{\mathbf{h}(\mathbf{x})}{ascii}: Reconstructs \mjeqn{\mathbf{x}}{ascii} 
from the latent representations.

- Auxiliary Function \mjqen{\mathbf{w}(\mathbf{u})}{ascii}: Estimates parameters of 
the autoregressive latent prior.

In particular, the auxiliary function gives spatio-temporal trend 
(\mjeqn{\mathbf{\mu}(\mathbf{s}, t)}{ascii}), variance 
(\mjeqn{\mathbf{\sigma}(\mathbf{s}, t)}{ascii}) 
and AR coefficient (\mjeqn{\mathbf{\gamma_r}(\mathbf{s}, t), 
r=1,\dots, W}{ascii}) functions. 
The parameter \mjeqn{W}{ascii} in the equations correspond to 
the parameter \code{ar_order} 
set by user, \mjeqn{\mathbf{s}}{ascii} is the spatial location 
and \mjeqn{t} is the 
temporal location. The latent components \mjeqn{z_1, \dots, z_P}{ascii} are 
assumed to be generated through the following autoregressive process:

\mjeqn{\mathbf{z}^t_i = \mu_i(\mathbf{s}, t) + \sum_{r=1}^{W}
 \gamma_{i,r}(\mathbf{s}, t) (\mathbf{z}_i^{t-r} - \mu_i(\mathbf{s}, t - r)) 
+ \epsilon_i(\mathbf{s}, t)}{ascii}

where \mjeqn{\epsilon_i \sim \mathcal{N}(\mathbf{0}, \sigma(\mathbf{s}, t))}{ascii} 
is Gaussian noise.

The model optimizes the **evidence lower bound (ELBO)**:

\mjeqn{\mathcal{L} = \mathbb{E}_{q(\mathbf{z} | \mathbf{x}, \mathbf{u})} \left[ \log p(\mathbf{x} | \mathbf{z}) \right] - D_{\text{KL}}(q(\mathbf{z} | \mathbf{x}, \mathbf{u}) \| p(\mathbf{z} | \mathbf{u}))}{ascii}

where:
- The first term maximizes reconstruction accuracy by ensuring 
\mjeqn{\mathbf{x}}{ascii} can be recovered from \mjeqn{\mathbf{z}}{ascii}.
- The second term regularizes the latent space, enforcing an 
autoregressive prior structure through \mjeqn{\mathbf{w}(\mathbf{u})}{ascii}.

The framework is implemented using deep neural networks, optimizing 
via stochastic gradient descent. This approach ensures latent variables 
retain meaningful spatio-temporal dependencies, improving predictive 
performance in complex datasets.
}
\examples{
p <- 3
n_time <- 100
n_spat <- 50
coords_time <- cbind(
    rep(runif(n_spat), n_time), rep(runif(n_spat), n_time),
    rep(1:n_time, each = n_spat)
)
data_obj <- generate_nonstationary_spatio_temporal_data_by_segments(
    n_time,
    n_spat, p, 5, 10, coords_time
)
latent_data <- data_obj$data
# Generate artificial observed data by applying a nonlinear mixture
obs_data <- mix_data(latent_data, 2)

# Increase the number of epochs for better performance.
resiVAE <- iVAEar_radial(
  data = obs_data, 
  spatial_locations = coords_time[, 1:2],
  time_points = coords_time[, 3],
  latent_dim = p, 
  n_s = n_spat,
  epochs = 1,
  batch_size = 64
)
cormat <- cor(resiVAE$IC, latent_data)
cormat
absolute_mean_correlation(cormat)

}
